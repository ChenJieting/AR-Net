<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }

  h1 {
    font-weight:300;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>
<!-- Start : Google Analytics Code -->
<!-- <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-64069893-2', 'auto');
  ga('send', 'pageview');
</script> -->
<!-- End : Google Analytics Code -->
<script type="text/javascript" src="resources/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <head>
    <link rel="icon" type="image/png" href="resources/ucsd_logo.png">
    <title>AR-Net: Adaptive Frame Resolution for Efficient Action Recognition</title>
    <meta property='og:title' content='AR-Net: Adaptive Frame Resolution for Efficient Action Recognition' />
    <meta property="og:description" content="Meng, Lin, Panda, Sattigeri, Karlinsky, Oliva, Saenko, Feris. AR-Net: Adaptive Frame Resolution for Efficient Action Recognition. In ECCV, 2020." />
    <meta property='og:url' content='https://mengyuest.github.io/AR-Net/' />
  </head>

  <body>
        <br>
        <center><span style="font-size:40px;font-weight:bold;color:#182B49">AR-Net: Adaptive Frame Resolution for <br/> Efficient Action Recognition</span></center>

        <table align=center width=900px>
          <tr>
            <td align=center width=180px>
            <center><span style="font-size:24px"><a href="https://mengyuest.github.io/" target="_blank">Yue
            Meng</a></span></center></td>
            <td align=center width=180px>
            <center><span style="font-size:24px"><a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-cclin" target="_blank">Chung-Ching Lin </a></span></center></td>
            <td align=center width=180px>
            <center><span style="font-size:24px"><a href="https://rpand002.github.io/" target="_blank">Rameswar Panda</a></span></center></td>
            <td align=center width=180px>
            <center><span style="font-size:24px"><a href="https://pronics2004.github.io/" target="_blank">Prasanna Sattigeri</a></span></center></td>
          <tr/>
         </table>

         <table align=center width=800px>
            <tr>
                <td align=center width=150px>
              <center><span style="font-size:24px"><a href="https://researcher.watson.ibm.com/researcher/view.php?person=il-LEONIDKA" target="_blank"> Leonid Karlinsky</a></span></center></td>
         <td align=center width=150px>
          <center><span style="font-size:24px"><a href="http://olivalab.mit.edu/audeoliva.html" target="_blank">Aude Oliva</a><sup>&#8225</sup></span></center></td>
        <td align=center width=150px>
          <center><span style="font-size:24px"><a href="http://ai.bu.edu/ksaenko.html" target="_blank">Kate Saenko</a><sup>&#8224</sup></span></center></td>
        <td align=center width=150px>
          <center><span style="font-size:24px"><a href="http://rogerioferis.com/" target="_blank">Rogerio Feris</a></span></center></td>
        <tr/>
      </table>

        <table align=center width=800px>
          <tr>
            <td align=center width=700px><center><span style="font-size:24px">MIT-IBM Watson AI Lab, IBM Research</span></center></td>
          <tr/>
            <tr>
            <td align=center width=700px><center><span style="font-size:24px">Boston University<sup>&#8224</sup></span></center></td>
            <tr/>
            <tr>
            <td align=center width=700px><center><span style="font-size:24px">Massachusetts Institute of Technology<sup>&#8225</sup></span></center></td>
            <tr/>

        </table>
        <table align=center width=400px>
          <tr>
            <td align=center width=150px>
            <center><span style="font-size:24px"><a href="https://eccv2020.eu/" target="_blank">ECCV 2020</a></span></center></td>
          <tr/>
        </table>
        <table align=center width=900px>
            <tr><td width=900px>
              <center><a href="resources/figure-2-v8-new.png"><img src = "resources/1_figure.png" height="250px"></img></a><br></center>
            </td></tr>
        </table>

        <center id="abstract"><h1>Abstract</h1></center>
        Action  recognition  is  an  open  and  challenging  problem  in computer  vision.  While  current  state-of-the-art  models  offer  excellent recognition results, their computational expense limits their impact for many real-world applications. In this paper, we propose a novel approach, called  AR-Net  (Adaptive  Resolution  Network),  that  selects  on-the-fly the optimal resolution for each frame conditioned on the input for efficient action recognition in long untrimmed videos. Specifically, given a video  frame,  a  policy  network  is  used  to  decide  what  input  resolution should be used for processing by the action recognition model, with the goal of improving both accuracy and efficiency. We efficiently train the policy network jointly with the recognition model using standard back-propagation. Extensive experiments on several challenging action recognition benchmark datasets well demonstrate the efficacy of our proposed approach over state-of-the-art methods.
        <br>
        <hr>



        <center id="overview"><h1>Network</h1></center>
        <table align=center width=1000px>
            <tr><td width=1000px>
              <center><a href="resources/network.png"><img src = "resources/2_network.png" height="400px"></img></a><br></center>
            </td></tr>
          </table>

        <br>
        <hr>

        <center id="results0"><h1>Action recognition results on ActivityNet-v1.3 and FCVID</h1></center>
        <table align=center width=1000px>
            <tr><td width=500px>
              <center><a href="resources/table.png"><img src = "resources/3_table.png" height="221px"></img></a><br></center>
            </td></tr>
          </table>

        <br>
        <hr>

        <center id="results1"><h1>Accuracy versus efficiency comparison</h1></center>
        <table align=center width=1000px>
            <tr><td width=500px>
              <center><a href="resources/graph.png"><img src = "resources/4_graph.png" height="400px"></img></a><br></center>
            </td></tr>
          </table>

        <br>
        <hr>

        <center id="sourceCode"><h1>Paper and Code</h1></center>


        <table align=center width=900px>
            <tr></tr>
          <tr>
            <td >
        <a href="https://github.com/mengyuest/AR-Net"><img class="paperpreview" src="resources/2_network.png" width="200px"/></a>
          </td>
          <td></td>
          <td width=700px > <span style="font-size:20px">
              Yue Meng, Chung-Ching Lin, Rameswar Panda, Prasanna Sattigeri, Leonid Karlinsky, Aude Oliva, Kate Saenko, and Rogerio Feris. <br/> <a href="https://mengyuest.github.io/AR-Net">ARNet: Adaptive Frame Resolution for Efficient Action Recognition</a> <br/> <i>European Conference on Computer Vision (ECCV)</i>, 2020 <br/>[<a href="https://mengyuest.github.io/AR-Net">PDF</a>][<a href="https://github.com/mengyuest/AR-Net">Code</a>]</span>
        </td>
        </tr>

        </table>
        </center>

      <br>
      <hr>

      <br/>

    <br><br>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
</body>
</html>